# PRIME: Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles

This repository hosts the source code and website for **PRIME**, an evaluation framework designed to evaluate how **social biases** influence **logical reasoning** in Large Language Models (LLMs).

PRIME introduces:
- A **logic puzzle based framework** for probing implicit bias  
- Automatic puzzle generation of **generic**, **stereotypical**, and **anti-stereotypical** variants  
- A dataset of **6,048 logic puzzles** for evaluating gender bias in deductive reasoning  
- Fine-grained metrics including **edit-distance** and a **bias difference** measure 
- A logic solver backed pipeline to ensure a puzzle with **minimal, solvable clue sets**

The PRIME project webpage is here:  
ðŸ‘‰ **https://prime-reasoning.github.io**

The dataset and code are available here:  
ðŸ‘‰ **https://github.com/FatimaJahara/PRIME**


---

## ðŸ“„ Citation

If you find PRIME useful for your research, please cite:

```
@misc{jahara2025evaluatingimplicitbiasesllm,
      title={Evaluating Implicit Biases in LLM Reasoning through Logic Grid Puzzles}, 
      author={Fatima Jahara and Mark Dredze and Sharon Levy},
      year={2025},
      eprint={2511.06160},
      archivePrefix={arXiv},
      primaryClass={cs.AI},
      url={https://arxiv.org/abs/2511.06160}, 
}
```

# Website License
<a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/"><img alt="Creative Commons License" style="border-width:0" src="https://i.creativecommons.org/l/by-sa/4.0/88x31.png" /></a><br />This work is licensed under a <a rel="license" href="http://creativecommons.org/licenses/by-sa/4.0/">Creative Commons Attribution-ShareAlike 4.0 International License</a>.

